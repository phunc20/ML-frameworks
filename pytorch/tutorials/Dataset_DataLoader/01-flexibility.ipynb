{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b569e83b",
   "metadata": {},
   "source": [
    "# `Dataset` and `DataLoader` from `torch.utils.data`\n",
    "On PyTorch's website, there are tutos on how to construct one's own customized dataset, e.g. [here](http://localhost:8888/tree/git-repos/phunc20/ML-frameworks/pytorch/tutorials/Dataset_DataLoader)\n",
    "\n",
    "However, it won't harm to add a few more remarks. The example given in the link above is as follows.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "```\n",
    "\n",
    "What I want to call to attention is that\n",
    "> this only gives **_an example_**, in the sense that we could follow the skeleton of this example but we are not obliged to structure our data with exactly the same directory structure on our hard disk.\n",
    "\n",
    "For instance, we do not need to always have a CSV file like in the code.\n",
    "\n",
    "In order to illustrate this, we will construct a simple dataset of ours together and we will learn the nitty-gritty along the way. To that end, I have hand-made a dataset in the folder `./pets/`, whose structure is like\n",
    "```\n",
    "$ tree pets/\n",
    "pets/\n",
    "└── train\n",
    "    ├── bong\n",
    "    │   ├── IMG-5547.jpg\n",
    "    │   ├── IMG-5548.jpg\n",
    "    │   └── IMG-5549.jpg\n",
    "    ├── meochi\n",
    "    │   ├── IMG-5533.jpg\n",
    "    │   ├── IMG-5534.jpg\n",
    "    │   └── IMG-5551.jpg\n",
    "    └── meoem\n",
    "        ├── IMG-5523.jpg\n",
    "        ├── IMG-5541.jpg\n",
    "        └── IMG-5542.jpg\n",
    "\n",
    "4 directories, 9 files\n",
    "```\n",
    "These are the photos of the pets I raise taken by the camera of a smart phone. I have either resized or rotated (or both) these images on purpose in order to\n",
    "\n",
    "- make them more real\n",
    "- anticipate some difficulties (as we will see shortly)\n",
    "\n",
    "**Rmk.** Of course, real-life image dataset won't contain so few images. I just hope that this toy example will help others and myself understand/review PyTorch's `Dataset` and `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c68e03",
   "metadata": {},
   "source": [
    "## Let's Get Started\n",
    "It seems that I forgot to mention the purpose of our dataset: It is created to train a model to distinguish btw each of our pets, named `bong`, `meochi`, `meoem`. But we won't train any model in this notebook; here we are just interested in how to setup a dataset for training in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c8c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
